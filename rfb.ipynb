{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, cross_validate, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, RocCurveDisplay, auc, roc_curve, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import expit as inverse_logit\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 18\n",
      "5 19\n",
      "6 20\n",
      "7 21\n",
      "8 22\n",
      "9 23\n"
     ]
    }
   ],
   "source": [
    "for i,v in zip(list(range(4,10)), list(range(18,24))):\n",
    "    print(i,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "start index is greater than length of predictor vector",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m start index is greater than length of predictor vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rae330/.conda/envs/pymc_env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "njobs = 200\n",
    "task_id = 152\n",
    "\n",
    "chunk_size = math.ceil(len(outcomes) / njobs)\n",
    "start = (chunk_size * (task_id))\n",
    "end = chunk_size * (task_id+1)\n",
    "if start > len(outcomes):\n",
    "    sys.exit('start index is greater than length of predictor vector')\n",
    "if end > len(outcomes):\n",
    "    end = len(outcomes)\n",
    "print('Start ', start)\n",
    "print('End ', end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('../tidy_data/proteomics_first_occurrences.parquet')\n",
    "proteins = pd.read_csv('../tidy_data/protein_colnames.txt', header=None)\n",
    "proteins = proteins[0].tolist()\n",
    "outcomes = pd.read_csv('../tidy_data/outcome_colnames.txt', header=None)\n",
    "outcomes = outcomes[0].tolist()\n",
    "demo = ['21003-0.0', '21003-0.0_squared', '31-0.0', '53-0.0', '54-0.0',]\n",
    "\n",
    "df['53-0.0'] = (df['53-0.0'] - min(df['53-0.0']))\n",
    "df['53-0.0'] = df['53-0.0'].apply(lambda x: x.days)\n",
    "\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(df.loc[:,['31-0.0', '54-0.0']])\n",
    "categ_enc = pd.DataFrame(enc.transform(df.loc[:,['31-0.0', '54-0.0']]).toarray(), columns = enc.get_feature_names_out(['31-0.0', '54-0.0']))\n",
    "\n",
    "df = df.drop(columns=['31-0.0', '54-0.0'])\n",
    "df = df.join(categ_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Iterate over outcomes\n",
    "create labels\n",
    "subset df to take all proteins, all demo variables, and outcome\n",
    "Iterate over 10 80% train/20% test splits, stratifying by label \n",
    "Iterate over number of features [5, 50, 100, 500, 1000, all], do 100 rounds of features except when doing all features\n",
    "choose proteins\n",
    "fit HistGradientBoostingClassifier, calculate AUROC, accuracy, balanced accuracy, precision, recall, F1\n",
    "Store confusion matrix, metrics, number of features, which train/test split\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rae330/torandy/rfb/code/rfb.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://127.0.0.1:50234/home/rae330/torandy/rfb/code/rfb.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m skf \u001b[39m=\u001b[39m StratifiedKFold(n_splits \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://127.0.0.1:50234/home/rae330/torandy/rfb/code/rfb.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m j, (train_index, test_index) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(skf\u001b[39m.\u001b[39msplit(X, y)):\n\u001b[0;32m---> <a href='vscode-notebook-cell://127.0.0.1:50234/home/rae330/torandy/rfb/code/rfb.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     clf \u001b[39m=\u001b[39m HistGradientBoostingClassifier(class_weight \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mbalanced\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(X\u001b[39m.\u001b[39;49miloc[train_index], y[train_index])\n\u001b[1;32m     <a href='vscode-notebook-cell://127.0.0.1:50234/home/rae330/torandy/rfb/code/rfb.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     test_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X\u001b[39m.\u001b[39miloc[test_index])\n\u001b[1;32m     <a href='vscode-notebook-cell://127.0.0.1:50234/home/rae330/torandy/rfb/code/rfb.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     tn, fp, fn, tp \u001b[39m=\u001b[39m confusion_matrix(y[test_index], test_pred)\u001b[39m.\u001b[39mravel()                \n",
      "File \u001b[0;32m~/.conda/envs/pymc_env/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pymc_env/lib/python3.12/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:674\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    666\u001b[0m n_bins \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_bins \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# + 1 for missing values\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bin_mapper \u001b[39m=\u001b[39m _BinMapper(\n\u001b[1;32m    668\u001b[0m     n_bins\u001b[39m=\u001b[39mn_bins,\n\u001b[1;32m    669\u001b[0m     is_categorical\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_categorical_remapped,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    672\u001b[0m     n_threads\u001b[39m=\u001b[39mn_threads,\n\u001b[1;32m    673\u001b[0m )\n\u001b[0;32m--> 674\u001b[0m X_binned_train \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bin_data(X_train, is_training_data\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    675\u001b[0m \u001b[39mif\u001b[39;00m X_val \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    676\u001b[0m     X_binned_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bin_data(X_val, is_training_data\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/pymc_env/lib/python3.12/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1189\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting._bin_data\u001b[0;34m(self, X, is_training_data)\u001b[0m\n\u001b[1;32m   1187\u001b[0m tic \u001b[39m=\u001b[39m time()\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m is_training_data:\n\u001b[0;32m-> 1189\u001b[0m     X_binned \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bin_mapper\u001b[39m.\u001b[39;49mfit_transform(X)  \u001b[39m# F-aligned array\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1191\u001b[0m     X_binned \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bin_mapper\u001b[39m.\u001b[39mtransform(X)  \u001b[39m# F-aligned array\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pymc_env/lib/python3.12/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/pymc_env/lib/python3.12/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis object (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m) has a `transform`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/.conda/envs/pymc_env/lib/python3.12/site-packages/sklearn/ensemble/_hist_gradient_boosting/binning.py:232\u001b[0m, in \u001b[0;36m_BinMapper.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m f_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_features):\n\u001b[1;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_categorical_[f_idx]:\n\u001b[0;32m--> 232\u001b[0m         thresholds \u001b[39m=\u001b[39m _find_binning_thresholds(X[:, f_idx], max_bins)\n\u001b[1;32m    233\u001b[0m         n_bins_non_missing\u001b[39m.\u001b[39mappend(thresholds\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    234\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m         \u001b[39m# Since categories are assumed to be encoded in\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         \u001b[39m# [0, n_cats] and since n_cats <= max_bins,\u001b[39;00m\n\u001b[1;32m    237\u001b[0m         \u001b[39m# the thresholds *are* the unique categorical values. This will\u001b[39;00m\n\u001b[1;32m    238\u001b[0m         \u001b[39m# lead to the correct mapping in transform()\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pymc_env/lib/python3.12/site-packages/sklearn/ensemble/_hist_gradient_boosting/binning.py:49\u001b[0m, in \u001b[0;36m_find_binning_thresholds\u001b[0;34m(col_data, max_bins)\u001b[0m\n\u001b[1;32m     47\u001b[0m     col_data \u001b[39m=\u001b[39m col_data[\u001b[39m~\u001b[39mmissing_mask]\n\u001b[1;32m     48\u001b[0m col_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mascontiguousarray(col_data, dtype\u001b[39m=\u001b[39mX_DTYPE)\n\u001b[0;32m---> 49\u001b[0m distinct_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49munique(col_data)\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(distinct_values) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m max_bins:\n\u001b[1;32m     51\u001b[0m     midpoints \u001b[39m=\u001b[39m distinct_values[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m distinct_values[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.conda/envs/pymc_env/lib/python3.12/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[1;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[1;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pymc_env/lib/python3.12/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[39m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     ar\u001b[39m.\u001b[39;49msort()\n\u001b[1;32m    337\u001b[0m     aux \u001b[39m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(aux\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mbool_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nfeats_l = []\n",
    "proteins_l = []\n",
    "outcome_l = []\n",
    "iter_l = []\n",
    "fold_l = []\n",
    "tn_l = []\n",
    "fp_l = []\n",
    "fn_l = []\n",
    "tp_l = []\n",
    "auroc_l = []\n",
    "prec_n = []\n",
    "prec_p = []\n",
    "rec_n = []\n",
    "rec_p = []\n",
    "f1_n = []\n",
    "f1_p = []\n",
    "\n",
    "demo_nosite_nosex = [x for x in demo if x not in ['31-0.0', '54-0.0']]\n",
    "\n",
    "for oc in outcomes:\n",
    "    \n",
    "    df['label'] = df[oc].notna().astype(int)\n",
    "    df_sub = df.loc[:, proteins + demo_nosite_nosex + ['label']]\n",
    "\n",
    "    y = df_sub.label\n",
    "    X_start = df_sub.drop(columns=['label'])    \n",
    "\n",
    "    for nfeat in [5, 50, 100, 500, 1000, len(proteins)]:\n",
    "        \n",
    "        if nfeat < len(proteins):\n",
    "            for i in range(100):\n",
    "\n",
    "                np.random.seed(seed=i)\n",
    "                p = np.random.choice(proteins, size = nfeat, replace = False)\n",
    "                p_drop = set(proteins).difference(set(p))\n",
    "                X = X_start.drop(columns = p_drop)\n",
    "                \n",
    "                skf = StratifiedKFold(n_splits = 10)\n",
    "                \n",
    "                for j, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "                    clf = HistGradientBoostingClassifier(class_weight = 'balanced').fit(X.iloc[train_index], y[train_index])\n",
    "\n",
    "                    test_pred = clf.predict(X.iloc[test_index])\n",
    "                    tn, fp, fn, tp = confusion_matrix(y[test_index], test_pred).ravel()                \n",
    "                    auroc = roc_auc_score(y[test_index], clf.predict_proba(X.iloc[test_index])[:,1])\n",
    "                    prfs = precision_recall_fscore_support(y[test_index], test_pred)\n",
    "                \n",
    "                    nfeats_l.append(nfeat)\n",
    "                    proteins_l.append(p)\n",
    "                    outcome_l.append(oc)\n",
    "                    iter_l.append(i)\n",
    "                    fold_l.append(j)\n",
    "                    tn_l.append(tn)\n",
    "                    fp_l.append(fp)\n",
    "                    fn_l.append(fn)\n",
    "                    tp_l.append(tp)\n",
    "                    auroc_l.append(auroc)\n",
    "                    prec_n.append(prfs[0][0])\n",
    "                    prec_p.append(prfs[0][1])\n",
    "                    rec_n.append(prfs[1][0])\n",
    "                    rec_p.append(prfs[1][1])\n",
    "                    f1_n.append(prfs[2][0])\n",
    "                    f1_p.append(prfs[2][1])\n",
    "        else:\n",
    "            np.random.seed(seed=0)\n",
    "            X = X_start\n",
    "            \n",
    "            skf = StratifiedKFold(n_splits = 10)\n",
    "            \n",
    "            for j, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "                clf = HistGradientBoostingClassifier(class_weight = 'balanced').fit(X.iloc[train_index], y[train_index])\n",
    "\n",
    "                test_pred = clf.predict(X.iloc[test_index])\n",
    "                tn, fp, fn, tp = confusion_matrix(y[test_index], test_pred).ravel()                \n",
    "                auroc = roc_auc_score(y[test_index], clf.predict_proba(X.iloc[test_index])[:,1])\n",
    "                prfs = precision_recall_fscore_support(y[test_index], test_pred)\n",
    "            \n",
    "                nfeats_l.append(nfeat)\n",
    "                proteins_l.append('all')\n",
    "                outcome_l.append(oc)\n",
    "                iter_l.append(0)\n",
    "                fold_l.append(j)\n",
    "                tn_l.append(tn)\n",
    "                fp_l.append(fp)\n",
    "                fn_l.append(fn)\n",
    "                tp_l.append(tp)\n",
    "                auroc_l.append(auroc)\n",
    "                prec_n.append(prfs[0][0])\n",
    "                prec_p.append(prfs[0][1])\n",
    "                rec_n.append(prfs[1][0])\n",
    "                rec_p.append(prfs[1][1])\n",
    "                f1_n.append(prfs[2][0])\n",
    "                f1_p.append(prfs[2][1])\n",
    "\n",
    "\n",
    "data = {'n_features': nfeats_l, 'proteins': proteins_l, 'outcome': outcome_l, 'iteration': iter_l, 'cv_fold': fold_l, 'TN': tn_l, 'FP': fp_l, 'FN': fn_l, 'TP': tp_l, 'AUROC': auroc_l,\n",
    "        'prec_neg': prec_n, 'prec_pos': prec_p, 'rec_neg': rec_n, 'rec_pos': rec_p, 'f1_neg': f1_n, 'f1_pos': f1_p}\n",
    "results = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'n_features': nfeats_l, 'proteins': proteins_l, 'outcome': outcome_l, 'iteration': iter_l, 'cv_fold': fold_l, 'TN': tn_l, 'FP': fp_l, 'FN': fn_l, 'TP': tp_l, 'AUROC': auroc_l,\n",
    "        'prec_neg': prec_n, 'prec_pos': prec_p, 'rec_neg': rec_n, 'rec_pos': rec_p, 'f1_neg': f1_n, 'f1_pos': f1_p}\n",
    "results = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_parquet('../tidy_data/results.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
